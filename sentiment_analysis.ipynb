{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SemEval 2017 task 4a "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My Pytorch version: 1.13.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from os.path import join\n",
    "from os.path import isfile\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import tabulate\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from transformers import BertTokenizer\n",
    "from transformers import BertModel\n",
    "\n",
    "\n",
    "print(\"My Pytorch version: \" + torch.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Defined Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Please change these as approprite\n",
    "\n",
    "DEVICE = torch.device('cuda') \n",
    "\n",
    "data_dir = \"data/\"\n",
    "\n",
    "testsets = ['twitter-test1.txt', 'twitter-test2.txt', 'twitter-test3.txt']\n",
    "devset = 'twitter-dev-data.txt'\n",
    "trainset = 'twitter-training-data.txt'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Don't change these\n",
    "\n",
    "CPU = torch.device('cpu')\n",
    "\n",
    "#Assigning intgers to each class\n",
    "NEG = 0\n",
    "NEUT = 1\n",
    "POS = 2\n",
    "\n",
    "LABELS_IDS = {'negative':NEG, 'neutral':NEUT, 'positive':POS}\n",
    "\n",
    "LABELS = {NEG:'negative',NEUT:'neutral',POS:'positive'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATION CODE\n",
    "def read_test(testset):\n",
    "    '''\n",
    "    readin the testset and return a dictionary\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    '''\n",
    "    id_gts = {}\n",
    "    with open(testset, 'r', encoding='utf8') as fh:\n",
    "        for line in fh:\n",
    "            fields = line.split('\\t')\n",
    "            tweetid = fields[0]\n",
    "            gt = fields[1]\n",
    "\n",
    "            id_gts[tweetid] = gt\n",
    "\n",
    "    return id_gts\n",
    "\n",
    "\n",
    "def confusion(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the confusion matrix of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    gts = []\n",
    "    for m, c1 in id_gts.items():\n",
    "        if c1 not in gts:\n",
    "            gts.append(c1)\n",
    "\n",
    "    gts = ['positive', 'negative', 'neutral']\n",
    "\n",
    "    conf = {}\n",
    "    for c1 in gts:\n",
    "        conf[c1] = {}\n",
    "        for c2 in gts:\n",
    "            conf[c1][c2] = 0\n",
    "\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            print(\"SHOULD NOT BE HERE\")\n",
    "            pred = 'neutral'\n",
    "        conf[pred][gt] += 1\n",
    "\n",
    "    print(''.ljust(12) + '  '.join(gts))\n",
    "\n",
    "    for c1 in gts:\n",
    "        print(c1.ljust(12), end='')\n",
    "        for c2 in gts:\n",
    "            if sum(conf[c1].values()) > 0:\n",
    "                print('%.3f     ' % (conf[c1][c2] / float(sum(conf[c1].values()))), end='')\n",
    "            else:\n",
    "                print('0.000     ', end='')\n",
    "        print('')\n",
    "\n",
    "    print('')\n",
    "\n",
    "\n",
    "def evaluate(id_preds, testset, classifier):\n",
    "    '''\n",
    "    print the macro-F1 score of {'positive', 'netative'} between preds and testset\n",
    "    :param id_preds: a dictionary of predictions formated as {<tweetid>:<sentiment>, ... }\n",
    "    :param testset: str, the file name of the testset to compare\n",
    "    :classifier: str, the name of the classifier\n",
    "    '''\n",
    "    id_gts = read_test(testset)\n",
    "\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    catf1s = {}\n",
    "\n",
    "    ok = 0\n",
    "    for tweetid, gt in id_gts.items():\n",
    "        if tweetid in id_preds:\n",
    "            pred = id_preds[tweetid]\n",
    "        else:\n",
    "            print(\"SHOULD NOT BE HERE\")\n",
    "            pred = 'neutral'\n",
    "\n",
    "        if gt == pred:\n",
    "            ok += 1\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    catcount = 0\n",
    "    itemcount = 0\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    micro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    semevalmacro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "\n",
    "    microtp = 0\n",
    "    microfp = 0\n",
    "    microtn = 0\n",
    "    microfn = 0\n",
    "    for cat, acc in acc_by_class.items():\n",
    "        catcount += 1\n",
    "\n",
    "        microtp += acc['tp']\n",
    "        microfp += acc['fp']\n",
    "        microtn += acc['tn']\n",
    "        microfn += acc['fn']\n",
    "\n",
    "        p = 0\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        catf1s[cat] = f1\n",
    "\n",
    "        n = acc['tp'] + acc['fn']\n",
    "\n",
    "        macro['p'] += p\n",
    "        macro['r'] += r\n",
    "        macro['f1'] += f1\n",
    "\n",
    "        if cat in ['positive', 'negative']:\n",
    "            semevalmacro['p'] += p\n",
    "            semevalmacro['r'] += r\n",
    "            semevalmacro['f1'] += f1\n",
    "\n",
    "        itemcount += n\n",
    "\n",
    "    micro['p'] = float(microtp) / float(microtp + microfp)\n",
    "    micro['r'] = float(microtp) / float(microtp + microfn)\n",
    "    micro['f1'] = 2 * float(micro['p']) * micro['r'] / float(micro['p'] + micro['r'])\n",
    "\n",
    "    semevalmacrof1 = semevalmacro['f1'] / 2\n",
    "\n",
    "    print(testset + ' (' + classifier + '): %.3f' % semevalmacrof1)\n",
    "\n",
    "    return semevalmacrof1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Simplified evaluation function used for development\n",
    "\n",
    "def evaluate_simple(predictions, true):\n",
    "    acc_by_class = {}\n",
    "    for gt in ['positive', 'negative', 'neutral']:\n",
    "        acc_by_class[gt] = {'tp': 0, 'fp': 0, 'tn': 0, 'fn': 0}\n",
    "\n",
    "    for i,gt in enumerate(true):\n",
    "        pred = predictions[i]\n",
    "        if gt == pred:\n",
    "            acc_by_class[gt]['tp'] += 1\n",
    "        else:\n",
    "            acc_by_class[gt]['fn'] += 1\n",
    "            acc_by_class[pred]['fp'] += 1\n",
    "\n",
    "    macro = {'p': 0, 'r': 0, 'f1': 0}\n",
    "    for cat, acc in acc_by_class.items():\n",
    "\n",
    "        p = 0\n",
    "        #Calculating precision\n",
    "        if (acc['tp'] + acc['fp']) > 0:\n",
    "            p = float(acc['tp']) / (acc['tp'] + acc['fp'])\n",
    "\n",
    "        r = 0\n",
    "        #Calculating recall\n",
    "        if (acc['tp'] + acc['fn']) > 0:\n",
    "            r = float(acc['tp']) / (acc['tp'] + acc['fn'])\n",
    "\n",
    "        f1 = 0 \n",
    "        # Calculating F1 scores\n",
    "        if (p + r) > 0:\n",
    "            f1 = 2 * p * r / (p + r)\n",
    "\n",
    "        #Adding scores for only the postive and negative classes\n",
    "        if cat in ['positive', 'negative']: \n",
    "            macro['p'] += p\n",
    "            macro['r'] += r\n",
    "            macro['f1'] += f1\n",
    "\n",
    "    macro_f1 = macro['f1'] / 2\n",
    "\n",
    "    return macro_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slang Replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing slang file\n",
    "\n",
    "slang_dict = {}\n",
    "\n",
    "with open(join(data_dir,'slang.txt'),'r') as f:\n",
    "    lines = f.readlines()\n",
    "for line in lines:\n",
    "    line = line.strip()\n",
    "    aslist = line.split('\\t')\n",
    "    if (len(aslist)!=2):\n",
    "        print(line)\n",
    "    slang = aslist[0]\n",
    "    replace = aslist[1]\n",
    "    slang_dict[slang]= replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_slang(tokens):\n",
    "    tokens_new = []\n",
    "    for token in tokens:\n",
    "        if token in slang_dict:\n",
    "            replacement= slang_dict[token]\n",
    "            tokens_to_add = replacement.split(\" \")\n",
    "            tokens_new+=tokens_to_add\n",
    "\n",
    "        else:\n",
    "            tokens_new.append(token)\n",
    "    \n",
    "    return tokens_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Emoji Replacement\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /dcs/pg22/u5501145/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "nltk.download('opinion_lexicon')\n",
    "\n",
    "pos_list=set(opinion_lexicon.positive())\n",
    "neg_list=set(opinion_lexicon.negative())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parsing Emoji Dictionary\n",
    "\n",
    "with open(join(data_dir,'Emoji_Dict.p'), 'rb') as f:\n",
    "    emoji_dict = pickle.load(f)\n",
    "    emoji_dict = {v: k for k, v in emoji_dict.items()} # Reverses dictionary so emojis are keys\n",
    " \n",
    "\n",
    "for k,v in emoji_dict.items(): #Extracts only sentiment words in emoji descriptions\n",
    "    raw = v.replace(\":\",\"\")\n",
    "    tokens = raw.split('_')\n",
    "    lem_list = [lemmatizer.lemmatize(t) for t in tokens] #Lemmatising first to give better chance of finding in wordlists\n",
    "    sentiment_tokens = [t for t in lem_list if t in pos_list or t in neg_list] # keeps only sentiment tokens\n",
    "    emoji_dict[k]=\" \".join(set(sentiment_tokens)) # using set to remove any duplicates\n",
    "                      \n",
    "    \n",
    "def convert_emojis(text): # Checks each word in text if its in the dict and replaces it if found\n",
    "    for word in text.split(' '):\n",
    "        if word in emoji_dict:\n",
    "            text = text.replace(word,emoji_dict[word])\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "emoji_1: smiling \n",
      "emoji_2: joy \n",
      "emoji_3: disappointed\n"
     ]
    }
   ],
   "source": [
    "print(convert_emojis(\"emoji_1: ðŸ˜Š \\nemoji_2: ðŸ˜‚ \\nemoji_3: ðŸ˜ž\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regular Expressions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing regular expressions:\n",
      "\n",
      "happy happy happy happy\n",
      "sad sad sad sad\n",
      " shouldbehere @ \n",
      "  shouldbehere and so should this  https://  \n",
      "this should  not contain any special symbols\n",
      "firstword   shouldbehere   thirdword \n",
      "5pm  42in \n",
      "yayy i loove itt\n"
     ]
    }
   ],
   "source": [
    "#Patterns\n",
    "\n",
    "pos_emoticons = r'(:|;|x|X|=)(\\)|D|p|P|3)'\n",
    "neg_emoticons = r'(:|;|=)(\\(|\\{|\\[)'\n",
    "mentions = r\"@[^\\s]+\"\n",
    "websites= r\"https?:\\/\\/[^\\s]+|www\\.[^\\s]+|[^\\s]+\\.(com|net|org|uk|ru|ir|in)|[^\\s]+\\.co\\.[^ ]{2}\"\n",
    "nonstandard_chars= r\"[^a-zA-Z0-9\\s]\" \n",
    "one_character_words= r\"\\b\\w{1}\\b\"\n",
    "numeric_words= r\"\\b[0-9]+\\b\"\n",
    "repeated_chars = r\"(.)\\1{2,}\"\n",
    "\n",
    "\n",
    "#Tests\n",
    "\n",
    "pos_emote_test = \":) ;) XD =D\"\n",
    "neg_emote_test = \":( ;( ={ =[\"\n",
    "mention_test = \"@shouldntbehere shouldbehere @ @s\"\n",
    "website_test = \"https://shouldntbehere.com www.shouldntbehere.com shouldbehere and so should this shouldn'tbehere.com https:// another.net anothertwice.co.nr\"\n",
    "nonstandard_test = \"thi{}s $shou#ld ! not conta@in {any spÂ£ecial symb!ols\"\n",
    "one_character_test = \"firstword h 4 shouldbehere a s thirdword b\"\n",
    "numeric_word_test = \"5pm 5764 42in 5\"\n",
    "repeated_chars_test = \"yayyyy i loooove itttt\"\n",
    "\n",
    "#patterns variable is a list of (pattern, replacement) for use with re.sub\n",
    "patterns = [(pos_emoticons,'happy'), (neg_emoticons,'sad'),(mentions, ''),(websites,''), (nonstandard_chars,''), (one_character_words,''), (numeric_words, ''),(repeated_chars,r'\\1\\1')]\n",
    "test_strings = [pos_emote_test, neg_emote_test,mention_test,website_test, nonstandard_test, one_character_test, numeric_word_test,repeated_chars_test]\n",
    "\n",
    "print(\"Testing regular expressions:\\n\")\n",
    "for i in range(len(patterns)):\n",
    "    pattern,replacement = patterns[i]\n",
    "    test_string = test_strings[i]\n",
    "    result = re.sub(pattern, replacement, test_string)\n",
    "    print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Performs regular expression substitution on patterns defined above\n",
    "def clean_text(text): \n",
    "    for pattern,replacement in patterns:\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Converts Penn Treebank POS tags to wordnet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    elif treebank_tag.startswith('S'):\n",
    "        return wordnet.ADJ_SAT\n",
    "    else:\n",
    "        return wordnet.NOUN\n",
    "\n",
    "#Applies all preprocessing steps\n",
    "def preprocess_text(text): \n",
    "    text = text.lower()\n",
    "    text = convert_emojis(text)\n",
    "    text = clean_text(text)\n",
    "\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = convert_slang(tokens)\n",
    "\n",
    "    tags = pos_tag(tokens)\n",
    "    #Lemmatisation using pos_tag, converted from penn treebank to wordnet\n",
    "    tokens = [lemmatizer.lemmatize(token, get_wordnet_pos(tag[1])) for token,tag in zip(tokens,tags)]\n",
    "    \n",
    "    out = \" \".join(tokens) # Untokenize (tokenization needs to be done differently for different features/models)\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading Data and applying pre-processing\n",
    "\n",
    "This cell takes some time to run (~2 min on my laptop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tweetids = {}\n",
    "tweetlabels = {}\n",
    "processed_tweets = {}\n",
    "raw_tweets = {}\n",
    "\n",
    "for filename in [trainset,devset]+testsets:\n",
    "    \n",
    "    path = join(data_dir,filename)\n",
    "    \n",
    "    with open(path) as file: \n",
    "        lines = file.readlines()\n",
    "    \n",
    "    data_all = [line.split(\"\\t\") for line in lines] # Each entry contains three attributes delimited by \\t\n",
    "    \n",
    "    tweetids[filename] = [d[0] for d in data_all]\n",
    "    tweetlabels[filename] = [d[1] for d in data_all]\n",
    "    raw_tweets[filename]= [d[2] for d in data_all]\n",
    "    processed_tweets[filename] = [preprocess_text(d[2]) for d in data_all] \n",
    "    \n",
    "data_tuple = (tweetids, tweetlabels, raw_tweets, processed_tweets)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data browser\n",
    "\n",
    "Set `active=True` to allow you browse through data, see raw and processed form.\\\n",
    "To see next example enter any input and press enter\\\n",
    "To stop, enter an empty input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "active = False\n",
    "\n",
    "i=55\n",
    "while(active):\n",
    "    user_input = input()\n",
    "    if (user_input==''):\n",
    "        break\n",
    "    else:\n",
    "        print(raw_tweets['twitter-training-data.txt'][i])\n",
    "        print(processed_tweets['twitter-training-data.txt'][i])\n",
    "        print(tweetlabels['twitter-training-data.txt'][i])\n",
    "        i+=1\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for linear classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bag of Words features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we fit the tfidf vectorizer on the training set \\\n",
    "The trained vectorizer will be used on both training and testing set using `tdif_features()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained tfidf vectorizer on training set\n"
     ]
    }
   ],
   "source": [
    "\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "training_tweets = processed_tweets[trainset]\n",
    "tfidf_vectorizer.fit(training_tweets)\n",
    "\n",
    "print(\"Fitted tfidf vectorizer on training set\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tfidf_features(tweets):\n",
    "    vectors = tfidf_vectorizer.transform(tweets)\n",
    "    return vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Glove features\n",
    "\n",
    "We first parse the pre-trained glove features from the txt file\\\n",
    "Then setup two functions: \n",
    "* `get_glove_vector()` takes a token and returns a vector, while dealing with OOV tokens\n",
    "* `glove_features()` takes a list of tweets and returns the mean glove vector for each\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parising glove vector file into a dictionary\n",
    "\n",
    "glove_dict = {}\n",
    "EMBEDDING_DIM = 100\n",
    "\n",
    "with open(join(data_dir,'glove.6B.100d.txt')) as f:\n",
    "    #line = f.readlines()[0]\n",
    "    for line in f.readlines():\n",
    "        aslist = line.split(\" \")\n",
    "        word = aslist[0]\n",
    "        vector_raw = aslist[1:]\n",
    "        vector = [float(n.strip()) for n in vector_raw]\n",
    "        glove_dict[word]=vector\n",
    "\n",
    "MEAN_GLOVE = np.mean(list(glove_dict.values()),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_glove_vector(token): # Simple funciton to deal with OOV words in glove\n",
    "    if token in glove_dict:\n",
    "        vector = glove_dict[token]\n",
    "    else:\n",
    "        vector = MEAN_GLOVE\n",
    "        \n",
    "    return vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glove_features(tweets):\n",
    "    tokenized_tweets = [word_tokenize(tweet) for tweet in tweets]\n",
    "    avg_vectors = np.zeros((len(tokenized_tweets),EMBEDDING_DIM))\n",
    "    for i,tokens in enumerate(tokenized_tweets):\n",
    "        if len(tokens)!=0:\n",
    "            vectors = [get_glove_vector(token) for token in tokens]\n",
    "            avg_vector = np.mean(vectors,axis=0)\n",
    "            avg_vectors[i] = avg_vector\n",
    "    return avg_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lexicon Features \n",
    "\n",
    "We use opinion lexicon from earlier and compute a vector of size 2 representing the average presence in postitive and negative word lists respectively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lexicon_features(tweets):\n",
    "    tokenized_tweets = [word_tokenize(tweet) for tweet in tweets]\n",
    "    avg_vectors = np.zeros((len(tokenized_tweets),2)) \n",
    "    for i,tokens in enumerate(tokenized_tweets):\n",
    "        if len(tokens)!=0:\n",
    "            vectors = [[token in pos_list, token in neg_list] for token in tokens]\n",
    "            avg_vectors[i] = np.mean(vectors,axis=0)\n",
    "    return avg_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combination of features\n",
    "\n",
    "def glove_lexicon_features(tweets):\n",
    "    glove_vectors = glove_features(tweets)\n",
    "    lexicon_vectors = lexicon_features(tweets)\n",
    "    return np.concatenate([glove_vectors,lexicon_vectors],axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiments\n",
    "\n",
    "The cell below takes a few minutes to run, it trains 4 different classifiers on 3 different features, and evaluates them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting glove features\n",
      "(45101, 100)\n",
      "45101\n",
      "Training lr\n",
      "data/twitter-test1.txt (glove-lr): 0.440\n",
      "data/twitter-test2.txt (glove-lr): 0.438\n",
      "data/twitter-test3.txt (glove-lr): 0.473\n",
      "Training svm\n",
      "data/twitter-test1.txt (glove-svm): 0.440\n",
      "data/twitter-test2.txt (glove-svm): 0.458\n",
      "data/twitter-test3.txt (glove-svm): 0.462\n",
      "data/twitter-test1.txt (glove-gnb): 0.483\n",
      "data/twitter-test2.txt (glove-gnb): 0.474\n",
      "data/twitter-test3.txt (glove-gnb): 0.465\n",
      "Training rf\n",
      "data/twitter-test1.txt (glove-rf): 0.322\n",
      "data/twitter-test2.txt (glove-rf): 0.353\n",
      "data/twitter-test3.txt (glove-rf): 0.351\n",
      "Extracting glove_lexicon features\n",
      "(45101, 102)\n",
      "45101\n",
      "Training lr\n",
      "data/twitter-test1.txt (glove_lexicon-lr): 0.474\n",
      "data/twitter-test2.txt (glove_lexicon-lr): 0.496\n",
      "data/twitter-test3.txt (glove_lexicon-lr): 0.507\n",
      "Training svm\n",
      "data/twitter-test1.txt (glove_lexicon-svm): 0.552\n",
      "data/twitter-test2.txt (glove_lexicon-svm): 0.560\n",
      "data/twitter-test3.txt (glove_lexicon-svm): 0.541\n",
      "data/twitter-test1.txt (glove_lexicon-gnb): 0.534\n",
      "data/twitter-test2.txt (glove_lexicon-gnb): 0.534\n",
      "data/twitter-test3.txt (glove_lexicon-gnb): 0.523\n",
      "Training rf\n",
      "data/twitter-test1.txt (glove_lexicon-rf): 0.432\n",
      "data/twitter-test2.txt (glove_lexicon-rf): 0.452\n",
      "data/twitter-test3.txt (glove_lexicon-rf): 0.445\n",
      "Extracting lexicon features\n",
      "(45101, 2)\n",
      "45101\n",
      "Training lr\n",
      "data/twitter-test1.txt (lexicon-lr): 0.389\n",
      "data/twitter-test2.txt (lexicon-lr): 0.406\n",
      "data/twitter-test3.txt (lexicon-lr): 0.404\n",
      "Training svm\n",
      "data/twitter-test1.txt (lexicon-svm): 0.306\n",
      "data/twitter-test2.txt (lexicon-svm): 0.329\n",
      "data/twitter-test3.txt (lexicon-svm): 0.304\n",
      "data/twitter-test1.txt (lexicon-gnb): 0.391\n",
      "data/twitter-test2.txt (lexicon-gnb): 0.394\n",
      "data/twitter-test3.txt (lexicon-gnb): 0.387\n",
      "Training rf\n",
      "data/twitter-test1.txt (lexicon-rf): 0.469\n",
      "data/twitter-test2.txt (lexicon-rf): 0.458\n",
      "data/twitter-test3.txt (lexicon-rf): 0.476\n",
      "Extracting bow features\n",
      "(45101, 42513)\n",
      "45101\n",
      "Training lr\n",
      "data/twitter-test1.txt (bow-lr): 0.570\n",
      "data/twitter-test2.txt (bow-lr): 0.572\n",
      "data/twitter-test3.txt (bow-lr): 0.545\n",
      "Training svm\n",
      "data/twitter-test1.txt (bow-svm): 0.499\n",
      "data/twitter-test2.txt (bow-svm): 0.527\n",
      "data/twitter-test3.txt (bow-svm): 0.478\n"
     ]
    }
   ],
   "source": [
    "\n",
    "run_experiments = True \n",
    "\n",
    "feature_extractors = {\"glove\":glove_features, \"lexicon\":lexicon_features, \"glove_lexicon\":glove_lexicon_features, 'bow':tfidf_features}\n",
    "\n",
    "to_test=testsets\n",
    "\n",
    "total_avg = 0\n",
    "n=0\n",
    "\n",
    "if (run_experiments):\n",
    "\n",
    "    training_set = processed_tweets[trainset] \n",
    "    train_labels = tweetlabels[trainset]\n",
    "    \n",
    "    FEATURE_LIST = ['glove','glove_lexicon','lexicon','bow']\n",
    "\n",
    "    CLF_LIST = ['lr','svm','gnb','rf']\n",
    "\n",
    "    results = []\n",
    "\n",
    "\n",
    "    for i,feature in enumerate(FEATURE_LIST):\n",
    "        \n",
    "        \n",
    "        feature_extractor = feature_extractors[feature]\n",
    "        print(\"Extracting {} features\".format(feature))\n",
    "            \n",
    "        train_features = feature_extractor(training_set)\n",
    "        \n",
    "        print(train_features.shape)\n",
    "        print(len(train_labels))\n",
    "\n",
    "        row =[feature]\n",
    "        \n",
    "        for j,classifier in enumerate(CLF_LIST):\n",
    "            # Skeleton: Creation and training of the classifiers\n",
    "            if classifier == 'svm':\n",
    "                print('Training ' + classifier)\n",
    "                clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "                clf.fit(train_features, train_labels)\n",
    "                \n",
    "            elif classifier == 'rf':\n",
    "                if (feature!='bow'):\n",
    "                    print('Training ' + classifier)\n",
    "                    clf = RandomForestClassifier()\n",
    "                    clf.fit(train_features,train_labels)\n",
    "                else:\n",
    "                    clf=None\n",
    "            elif classifier == 'lr':\n",
    "                print('Training ' + classifier)\n",
    "                clf = LogisticRegression(solver = 'saga', max_iter=200)\n",
    "                clf.fit(train_features,train_labels)\n",
    "            elif classifier == 'gnb':\n",
    "                if (feature!='bow'):\n",
    "                    clf = GaussianNB()\n",
    "                    clf.fit(train_features,train_labels)\n",
    "                else:\n",
    "                    clf = None\n",
    "            else:\n",
    "                print('Unknown classifier name' + classifier)\n",
    "                continue\n",
    "\n",
    "            # Predition performance of the classifiers\n",
    "            if (clf!=None):\n",
    "                test_scores = []\n",
    "                for testset in to_test:\n",
    "                    \n",
    "                    testset_name = testset\n",
    "                    testset_path = join(data_dir, testset_name)\n",
    "                    \n",
    "                    test_set = processed_tweets[testset_name]\n",
    "                    test_ids = tweetids[testset_name]\n",
    "                    \n",
    "                    #print(\"Extracting testing features for \", testset_name)\n",
    "                    test_features = feature_extractor(test_set)\n",
    "                    \n",
    "                    #print(\"Predicting labels for \", testset_name)\n",
    "                    predictions = clf.predict(test_features)\n",
    "\n",
    "                    id_preds = {i:j for i,j in zip(test_ids,predictions)}\n",
    "                    \n",
    "                    score = evaluate(id_preds, testset_path, feature + '-' + classifier)\n",
    "\n",
    "                    test_scores.append(score)\n",
    "\n",
    "                row.append(np.mean(test_scores))\n",
    "                total_avg+=np.mean(test_scores)\n",
    "                n+=1\n",
    "        results.append(row)\n",
    "\n",
    "total_avg = total_avg/n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  lr    svm    gnb     rf\n",
      "-------------  -----  -----  -----  -----\n",
      "glove          0.450  0.453  0.474  0.342\n",
      "glove_lexicon  0.492  0.551  0.530  0.443\n",
      "lexicon        0.400  0.313  0.390  0.467\n",
      "bow            0.562  0.501\n",
      "average score is 0.4551056388061755\n"
     ]
    }
   ],
   "source": [
    "if (run_experiments):\n",
    "\n",
    "    print(tabulate.tabulate(results,headers= [\"\"]+CLF_LIST,floatfmt=\".3f\"))\n",
    "\n",
    "    print(\"average score is {}\".format(total_avg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            positive  negative  neutral\n",
      "positive    0.740     0.048     0.213     \n",
      "negative    0.093     0.814     0.093     \n",
      "neutral     0.248     0.157     0.594     \n",
      "\n",
      "data/twitter-test1.txt (bow-lr): 0.570\n",
      "            positive  negative  neutral\n",
      "positive    0.787     0.048     0.165     \n",
      "negative    0.120     0.723     0.157     \n",
      "neutral     0.336     0.109     0.555     \n",
      "\n",
      "data/twitter-test2.txt (bow-lr): 0.572\n",
      "            positive  negative  neutral\n",
      "positive    0.753     0.065     0.182     \n",
      "negative    0.214     0.658     0.128     \n",
      "neutral     0.291     0.133     0.575     \n",
      "\n",
      "data/twitter-test3.txt (bow-lr): 0.545\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Here we retrain the best model (TFIDF+LR (MaxEnt)) and provide more detailed results\n",
    "\n",
    "\n",
    "to_test = testsets\n",
    "\n",
    "training_set = processed_tweets['twitter-training-data.txt'] \n",
    "train_labels = tweetlabels['twitter-training-data.txt']\n",
    "\n",
    "train_features = tfidf_features(training_set)\n",
    "\n",
    "lr_clf = LogisticRegression(solver = 'saga', max_iter=200)\n",
    "\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "for testset in to_test:\n",
    "\n",
    "    test_tweets = processed_tweets[testset]\n",
    "    test_features = tfidf_features(test_tweets)\n",
    "    test_ids = tweetids[testset]\n",
    "\n",
    "    predictions = lr_clf.predict(test_features)\n",
    "    id_preds = {i:j for i,j in zip(test_ids,predictions)}\n",
    "\n",
    "    test_labels = tweetlabels[testset]\n",
    "\n",
    "    confusion(id_preds, join(data_dir,testset), \"bow\" + '-' + \"lr\")\n",
    "\n",
    "    evaluate(id_preds, join(data_dir,testset), \"bow\" + '-' + \"lr\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This performs a simple pre-padding\n",
    "\n",
    "def pad(sequence, desired_length, pad_value):\n",
    "    if (len(sequence))==desired_length:\n",
    "        return sequence\n",
    "    elif (len(sequence)<desired_length):\n",
    "        diff = desired_length-len(sequence)\n",
    "        return [pad_value for i in range(diff)]+sequence\n",
    "    else:\n",
    "        return sequence[:desired_length] #Cuts sequence to desired length if its longer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LSTM Tokenizer\n",
    "\n",
    "This class creates an object that can be fit to some training data, returning an embedding matrix, and transforms any data to correspond with that embeddings matrix.\n",
    "\n",
    "* `fit()` takes pre-processed textual data, embeddings dictionary of form {token:vector} and embedding dimension and returns embeddings matrix \\\n",
    "  It also computes and stores the vocabulary for use in the tranform method\n",
    "\n",
    "* `transform()` takes textual data and returns numeric tokens\\\n",
    "It uses the vocab computed by fit function to assign each token an index, which matches with the corresponding vector in the matrix returned by fit()\n",
    "\n",
    "Note: Fit function adds two further tokens '[pad]' and '[unk]' to the vocab and adds a zero vector and mean vector their repsecitve embeddings.\n",
    "\n",
    "To handle OOV tokens, the transform function will use the index of the '[unk]' vector, for any OOV tokens.\n",
    "\n",
    "Examples:\n",
    "\n",
    "\n",
    "`tokenizer.fit(['word','another','word'], glove_dict, 100)` returns a 4x100 embeddings matrix and stores the vocab\n",
    "`tokenizer.transform(['word','another','word']) returns [0, 1, 0]` where each value is an index pointing to the corresponding embedding in the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class LSTMTokenizer():\n",
    "    def __init__(self):\n",
    "        self.vocab_list = None\n",
    "        self.vocab_dict = None\n",
    "        self.matrix = None\n",
    "        self.max_length = None\n",
    "        \n",
    "\n",
    "    def fit(self,data, vector_dict, vector_dim, max_length = 56):\n",
    "\n",
    "        self.max_length = max_length\n",
    "        tokenized_data = [word_tokenize(tweet) for tweet in data]\n",
    "        all_tokens = [token for sequence in tokenized_data for token in sequence]\n",
    "        vocab_set = set(all_tokens)\n",
    "        self.vocab_list = [token for token in all_tokens if token in vocab_set]\n",
    "    \n",
    "\n",
    "        matrix = np.zeros((len(self.vocab_list),vector_dim))\n",
    "        for i,token in enumerate(self.vocab_list):\n",
    "            if (token in vector_dict):\n",
    "                matrix[i]=vector_dict[token]\n",
    "\n",
    "        self.vocab_list.append('[pad]')\n",
    "        self.vocab_list.append('[unk]')\n",
    "\n",
    "        self.vocab_dict = {token:index for index,token in enumerate(self.vocab_list)}\n",
    "\n",
    "        pad_vector = np.zeros((vector_dim))\n",
    "        unkown_vector = np.mean(matrix,axis=0)\n",
    "\n",
    "        matrix = np.concatenate([matrix, [pad_vector,unkown_vector]],axis=0)\n",
    "        self.matrix = matrix\n",
    "        \n",
    "        return matrix\n",
    "    \n",
    "    def transform(self,data):\n",
    "        \n",
    "        if (self.vocab_dict == None):\n",
    "            raise Exception(\"LSTM Tokenizer has not been fitted, cannot transform any data.\")\n",
    "\n",
    "        tokenized_data = [word_tokenize(tweet) for tweet in data]\n",
    "\n",
    "        tokenized_data = [pad(sequence, self.max_length, '[pad]') for sequence in tokenized_data]\n",
    "\n",
    "        numeric_tokens = np.zeros((len(data),self.max_length))\n",
    "\n",
    "        for i,sequence in enumerate(tokenized_data):\n",
    "            for j,token in enumerate(sequence):\n",
    "                if token not in self.vocab_dict:\n",
    "                    numeric_tokens[i,j] = self.vocab_dict['[unk]'] # Handling OOV words here\n",
    "                else:\n",
    "                    numeric_tokens[i,j] = self.vocab_dict[token]\n",
    "\n",
    "        return numeric_tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing Training Data for LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitted LSTM tokenizer to training data\n",
      "x_train (shape):  torch.Size([45101, 56])\n",
      "y_train (shape):  torch.Size([45101])\n",
      "x dev (shape):  torch.Size([2000, 56])\n",
      "y_dev (shape):  torch.Size([2000])\n",
      "Train and Dev Loaders done preparing\n"
     ]
    }
   ],
   "source": [
    "\n",
    "lstm_prepare=True\n",
    "\n",
    "tokenizer_filename = join(data_dir, 'lstm_tokenizer.p')\n",
    "matrix_filename = join(data_dir, 'embeddings_matrix.p')\n",
    "\n",
    "if lstm_prepare:\n",
    "\n",
    "    train_tweets = processed_tweets[trainset]\n",
    "    train_labels = tweetlabels[trainset]\n",
    "\n",
    "    dev_tweets  = processed_tweets[devset]\n",
    "    dev_labels = tweetlabels[devset]\n",
    "\n",
    "    lstm_tokenizer = LSTMTokenizer()\n",
    "\n",
    "    #Creating the embeddings matrix for the vocab of training set \n",
    "    embeddings_matrix = lstm_tokenizer.fit(train_tweets, glove_dict, EMBEDDING_DIM) \n",
    "\n",
    "    print(\"Fitted LSTM tokenizer to training data\")\n",
    "\n",
    "\n",
    "    #Data is transformed using the tokenizer here:\n",
    "    X_train = torch.tensor(lstm_tokenizer.transform(train_tweets)).long()\n",
    "    Y_train = torch.tensor([LABELS_IDS[label] for label in train_labels]).long()\n",
    "\n",
    "    X_dev = torch.tensor(lstm_tokenizer.transform(dev_tweets)).long()\n",
    "    Y_dev = torch.tensor([LABELS_IDS[label] for label in dev_labels]).long()\n",
    "\n",
    "\n",
    "    print(\"x_train (shape): \", X_train.shape)\n",
    "    print(\"y_train (shape): \", Y_train.shape)\n",
    "\n",
    "    print(\"x dev (shape): \", X_dev.shape)\n",
    "    print(\"y_dev (shape): \", Y_dev.shape)\n",
    "\n",
    "    BATCH_SIZE = 256\n",
    "\n",
    "    train_zipped = list(zip(X_train, Y_train))\n",
    "    dev_zipped = list(zip(X_dev, Y_dev))\n",
    "\n",
    "    lstm_train_loader = DataLoader(train_zipped, batch_size=BATCH_SIZE)\n",
    "    lstm_dev_loader  = DataLoader(dev_zipped, batch_size=BATCH_SIZE)\n",
    "\n",
    "    print(\"Train and Dev Loaders done preparing\")\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#These functions are used so that the LSTM and BERT models can use the same training loop\n",
    "#It takes a batch and splits it into its components according to what kind of model is being used\n",
    "\n",
    "def lstm_batch_splitter(batch):\n",
    "    return ([batch[0].to(DEVICE)],batch[1].to(DEVICE))\n",
    "\n",
    "def bert_batch_splitter(batch):\n",
    "    return ([batch[0].to(DEVICE),batch[1].to(DEVICE)],batch[2].to(DEVICE))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM_base(nn.Module):\n",
    "    def __init__(self, embeddings_matrix, embedding_dim):\n",
    "        super(LSTM_base, self).__init__()\n",
    "        \n",
    "        hidden_dim = 32\n",
    "        layers = 2 \n",
    "\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(embeddings_matrix).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, layers, bidirectional=False, batch_first =True, dropout=0.4)\n",
    "        self.linear = nn.Linear(hidden_dim, 3)\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "            \n",
    "    def forward(self, X_batch):\n",
    "        out = self.embedding(X_batch)\n",
    "        out,_ = self.lstm(out)\n",
    "        out = out[:, -1] # gets final outputs from LSTM\n",
    "        out = self.activation(out)\n",
    "        out = self.linear(out) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, embeddings_matrix, embedding_dim):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        \n",
    "        hidden_dim = 32\n",
    "        layers = 2\n",
    "\n",
    "        self.embedding = torch.nn.Embedding.from_pretrained(torch.from_numpy(embeddings_matrix).float())\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, layers, bidirectional=True, batch_first =True, dropout=0.4)\n",
    "        self.linear = nn.Linear(2*hidden_dim, 3)\n",
    "        self.activation = nn.Tanh()\n",
    "        self.softmax = nn.Softmax(1)\n",
    "\n",
    "            \n",
    "    def forward(self, X_batch):\n",
    "        out = self.embedding(X_batch)\n",
    "        out,_ = self.lstm(out)\n",
    "        out = out[:, -1] # Gets final output from LSTM\n",
    "        out = self.activation(out)\n",
    "        out = self.linear(out) \n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, batch_splitter, loss_fn, val_loader):\n",
    "    with torch.no_grad():\n",
    "        labels, predictions, losses = [],[],[]\n",
    "        for batch in val_loader:\n",
    "            input_args, Y = batch_splitter(batch)\n",
    "            preds = model(*input_args)\n",
    "            loss  = loss_fn(preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            labels.append(Y)\n",
    "            predictions.append(preds.argmax(dim=-1))\n",
    "\n",
    "        #Flatten the labels and predictions\n",
    "        labels_flat = torch.cat(labels).to(CPU).numpy()\n",
    "        predictions_flat = torch.cat(predictions).to(CPU).numpy()\n",
    "\n",
    "        mean_loss = np.mean(losses)\n",
    "        f1 = evaluate_simple([LABELS[label] for label in labels_flat], [LABELS[pred] for pred in predictions_flat])\n",
    "        print(\"Validation Loss : {:.3f}\".format(mean_loss))\n",
    "        print(\"Validation SEMEVAL F1  : {:.3f}\".format(f1))\n",
    "        \n",
    "        return mean_loss,f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(model, batch_splitter, loss_fn, optimizer, train_loader, val_loader, epochs=10):\n",
    "    training_losses = []\n",
    "    training_f1s = []\n",
    "    validation_losses = []\n",
    "    validation_f1s = []\n",
    "\n",
    "    for i in range(1, epochs+1):\n",
    "        losses, predictions, labels = [],[],[]\n",
    "\n",
    "        for batch in tqdm(train_loader):\n",
    "            \n",
    "            input_args, Y = batch_splitter(batch)\n",
    "\n",
    "            Y_preds = model(*input_args)\n",
    "\n",
    "            \n",
    "            loss = loss_fn(Y_preds, Y)\n",
    "            losses.append(loss.item())\n",
    "\n",
    "            Y_pred_labels = Y_preds.argmax(dim=-1)\n",
    "            predictions.append(Y_pred_labels)\n",
    "            \n",
    "            labels.append(Y)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()            \n",
    "            optimizer.step()\n",
    "\n",
    "        \n",
    "        labels_flat = torch.cat(labels).to(CPU).detach().numpy()\n",
    "        predictions_flat = torch.cat(predictions).to(CPU).detach().numpy()\n",
    "        mean_train_loss = np.mean(losses)\n",
    "\n",
    "        train_f1 = evaluate_simple([LABELS[label] for label in labels_flat], [LABELS[pred] for pred in predictions_flat])\n",
    "        print(\"Train Loss : {:.3f}\".format(mean_train_loss))\n",
    "        print(\"Training SEMEVAL F1 : {:.3f}\".format(train_f1))\n",
    "        val_loss, val_f1= validate(model, batch_splitter, loss_fn, val_loader)\n",
    "\n",
    "        training_losses.append(mean_train_loss)\n",
    "        training_f1s.append(train_f1)\n",
    "        validation_losses.append(val_loss)\n",
    "        validation_f1s.append(val_f1)\n",
    "\n",
    "    return (training_losses,training_f1s,validation_losses,validation_f1s)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training LSTM-base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model from file\n",
      "Loaded successfuly\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "lstm_base_filename = join(data_dir,'lstm_base.p')\n",
    "\n",
    "if not train:\n",
    "    print(\"Attempting to load model from file\")\n",
    "    lstm_base = torch.load(lstm_base_filename)\n",
    "    print(\"Loaded successfuly\")\n",
    "else:\n",
    "\n",
    "    lstm_base = LSTM_base(embeddings_matrix,EMBEDDING_DIM).to(DEVICE)\n",
    "\n",
    "    EPOCHS=30\n",
    "    LEARN_RATE = 1e-4\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(weight = torch.tensor([1,0.5,1]).to(DEVICE))\n",
    "\n",
    "    optimizer = Adam(lstm_base.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    #Train loop returns training statistics for use in the next cell\n",
    "    tl, ta, vl, va = train_loop(lstm_base, lstm_batch_splitter, loss_fn, optimizer, lstm_train_loader, lstm_dev_loader, EPOCHS)\n",
    "\n",
    "    print(\"Saving trained model\")\n",
    "\n",
    "\n",
    "    torch.save(lstm_base.to(CPU), lstm_base_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#If training was just performed we can plot the training graphs here\n",
    "\n",
    "if (train):\n",
    "    plt.figure(figsize=(12,5))\n",
    "    plt.subplot(1,2,1)\n",
    "    plt.plot(tl,label = 'Training Loss')\n",
    "    plt.plot(vl, label = 'Validation Loss')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(1,2,2)\n",
    "    plt.plot(ta, label = 'Training F1')\n",
    "    plt.plot(va, label = 'Validation F1')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating LSTM-Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/twitter-test1.txt (lstm-base): 0.599\n",
      "            positive  negative  neutral\n",
      "positive    0.574     0.079     0.347     \n",
      "negative    0.146     0.638     0.215     \n",
      "neutral     0.171     0.148     0.681     \n",
      "\n",
      "data/twitter-test2.txt (lstm-base): 0.575\n",
      "            positive  negative  neutral\n",
      "positive    0.635     0.072     0.294     \n",
      "negative    0.136     0.612     0.252     \n",
      "neutral     0.273     0.106     0.620     \n",
      "\n",
      "data/twitter-test3.txt (lstm-base): 0.578\n",
      "            positive  negative  neutral\n",
      "positive    0.614     0.082     0.305     \n",
      "negative    0.179     0.481     0.340     \n",
      "neutral     0.209     0.132     0.659     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "to_test = testsets\n",
    "\n",
    "lstm_base.to(DEVICE)\n",
    "lstm_base.eval()\n",
    "\n",
    "\n",
    "for testset in to_test:\n",
    "\n",
    "    test_tweets = processed_tweets[testset]\n",
    "    test_labels = tweetlabels[testset]\n",
    "    test_ids = tweetids[testset]\n",
    "\n",
    "    X_test = torch.tensor(lstm_tokenizer.transform(test_tweets)).long().to(DEVICE)\n",
    "    Y_test = torch.tensor([LABELS_IDS[label] for label in test_labels]).long().to(DEVICE)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pred = lstm_base(X_test)\n",
    "        \n",
    "        test_pred_labels = test_pred.argmax(dim=-1)\n",
    "\n",
    "        predictions = [LABELS[i.item()] for i in test_pred_labels]\n",
    "\n",
    "        pred_dict = {id:label for id,label in zip(test_ids,predictions)}\n",
    "\n",
    "\n",
    "    evaluate(pred_dict, join(data_dir,testset), \"lstm-base\")\n",
    "\n",
    "    confusion(pred_dict, join(data_dir,testset), \"lstm-base\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load model from file\n",
      "Loaded successfully\n"
     ]
    }
   ],
   "source": [
    "train = False\n",
    "bilstm_filename = join(data_dir,'bilstm.p')\n",
    "\n",
    "if not train:\n",
    "    print(\"Attempting to load model from file\")\n",
    "    bi_lstm = torch.load(bilstm_filename)\n",
    "    print(\"Loaded successfully\")\n",
    "else:\n",
    "\n",
    "    bi_lstm = BiLSTM(embeddings_matrix,EMBEDDING_DIM).to(DEVICE)\n",
    "\n",
    "    EPOCHS= 30\n",
    "    LEARN_RATE = 1e-4\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(weight=torch.tensor([1,0.5,1]).to(DEVICE))\n",
    "\n",
    "    optimizer = Adam(bi_lstm.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "    train_losses = []\n",
    "    valid_losses = []\n",
    "\n",
    "    tl, ta, vl, va = train_loop(bi_lstm, lstm_batch_splitter, loss_fn, optimizer, lstm_train_loader, lstm_dev_loader, EPOCHS)\n",
    "\n",
    "    print(\"Saving trained model\")\n",
    "\n",
    "    torch.save(bi_lstm.to(CPU),bilstm_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Bi-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/twitter-test1.txt (bi-lstm): 0.598\n",
      "            positive  negative  neutral\n",
      "positive    0.577     0.082     0.342     \n",
      "negative    0.130     0.683     0.187     \n",
      "neutral     0.172     0.149     0.679     \n",
      "\n",
      "data/twitter-test2.txt (bi-lstm): 0.575\n",
      "            positive  negative  neutral\n",
      "positive    0.644     0.071     0.284     \n",
      "negative    0.115     0.596     0.288     \n",
      "neutral     0.254     0.109     0.637     \n",
      "\n",
      "data/twitter-test3.txt (bi-lstm): 0.594\n",
      "            positive  negative  neutral\n",
      "positive    0.613     0.082     0.305     \n",
      "negative    0.175     0.538     0.287     \n",
      "neutral     0.197     0.125     0.678     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "to_test = testsets\n",
    "\n",
    "bi_lstm.to(DEVICE)\n",
    "bi_lstm.eval()\n",
    "\n",
    "for testset in to_test:\n",
    "\n",
    "    test_tweets = processed_tweets[testset]\n",
    "    test_labels = tweetlabels[testset]\n",
    "    test_ids = tweetids[testset]\n",
    "\n",
    "    X_test = torch.tensor(lstm_tokenizer.transform(test_tweets)).long().to(DEVICE)\n",
    "    Y_test = torch.tensor([LABELS_IDS[label] for label in test_labels]).long().to(DEVICE)\n",
    "\n",
    "\n",
    "\n",
    "    with torch.no_grad():\n",
    "        test_pred = bi_lstm(X_test.to(DEVICE))\n",
    "        \n",
    "        test_pred_labels = test_pred.argmax(dim=-1)\n",
    "\n",
    "        predictions = [LABELS[i.item()] for i in test_pred_labels]\n",
    "\n",
    "        pred_dict = {id:label for id,label in zip(test_ids,predictions)}\n",
    "\n",
    "    evaluate(pred_dict, join(data_dir,testset), \"bi-lstm\")\n",
    "    confusion(pred_dict,join(data_dir,testset),\"bi-lstm\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Based Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing training data for BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Tokenizer *must* be retrieved here for evaluation as well as training\n",
    "bert_tokenizer = BertTokenizer.from_pretrained('bert-base-cased')\n",
    "\n",
    "\n",
    "bert_prepare= False # Only set True if you need to train the model\n",
    "\n",
    "#If bert_prepare is true we prepare training and dev data to use in the training\n",
    "if (bert_prepare):\n",
    "\n",
    "    train_tweets = processed_tweets[trainset]\n",
    "    train_labels = tweetlabels[trainset]\n",
    "\n",
    "    dev_tweets = processed_tweets[devset]\n",
    "    dev_labels = tweetlabels[devset]\n",
    "\n",
    "    X_train = bert_tokenizer(train_tweets,padding=True, truncation=True, max_length=40)\n",
    "    Y_train = torch.tensor([LABELS_IDS[label] for label in train_labels]).long() \n",
    "\n",
    "    X_dev = bert_tokenizer(dev_tweets,padding=True, truncation=True, max_length=40)\n",
    "    Y_dev = torch.tensor([LABELS_IDS[label] for label in dev_labels]).long()\n",
    "    ids_train = torch.tensor(X_train['input_ids'])\n",
    "    masks_train = torch.tensor(X_train['attention_mask'])\n",
    "\n",
    "    ids_dev = torch.tensor(X_dev['input_ids'])\n",
    "    masks_dev = torch.tensor(X_dev['attention_mask'])\n",
    "\n",
    "    train_zipped = list(zip(ids_train, masks_train, Y_train))\n",
    "    dev_zipped = list(zip(ids_dev, masks_dev, Y_dev))\n",
    "\n",
    "    bert_train_loader = DataLoader(train_zipped, batch_size=256)\n",
    "    bert_dev_loader  = DataLoader(dev_zipped, batch_size=256)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = BertModel.from_pretrained('bert-base-cased')\n",
    "        self.bert.requires_grad = False\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, 3) # Bert outputs 768 dimensional encodings\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        out = self.dropout(pooled_output)\n",
    "        out = self.linear(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training BERT-based model\n",
    "\n",
    "This requires a GPU, to use it set `DEVICE=torch.device('cuda')` in the user defined constants section\\\n",
    "Training took around 6 minutes for 3 epochs on the the lab computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded bert classifier from file\n"
     ]
    }
   ],
   "source": [
    "train=False\n",
    "\n",
    "bert_filename = join(data_dir,'bert-classifier.p') \n",
    "\n",
    "if (train):\n",
    "\n",
    "    bert_classifier = BertClassifier().to(DEVICE)\n",
    "\n",
    "    EPOCHS= 3\n",
    "    LEARN_RATE = 1e-4\n",
    "\n",
    "\n",
    "    loss_fn = nn.CrossEntropyLoss(weight = torch.tensor([1,0.5,1]).to(DEVICE))\n",
    "\n",
    "    optimizer = Adam(bert_classifier.parameters(), lr=LEARN_RATE)\n",
    "\n",
    "    tl, ta, vl, va = train_loop(bert_classifier, bert_batch_splitter, loss_fn, optimizer, bert_train_loader, bert_dev_loader, EPOCHS)\n",
    "\n",
    "    torch.save(bert_classifier.to(CPU),bert_filename)\n",
    "\n",
    "else:\n",
    "    bert_classifier = torch.load(bert_filename)\n",
    "\n",
    "    print(\"Loaded bert classifier from file\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating BERT-based model\n",
    "\n",
    "GPU is reccomended for evaluation. It can be done on cpu but took around 15 minutes for one dataset\n",
    "\n",
    "To use GPU, set `DEVICE=torch.device('cuda')` in the user defined constants section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/twitter-test1.txt (BERT+Linear): 0.707\n",
      "            positive  negative  neutral\n",
      "positive    0.708     0.032     0.260     \n",
      "negative    0.066     0.789     0.145     \n",
      "neutral     0.189     0.124     0.688     \n",
      "\n",
      "data/twitter-test2.txt (BERT+Linear): 0.698\n",
      "            positive  negative  neutral\n",
      "positive    0.769     0.028     0.202     \n",
      "negative    0.065     0.748     0.187     \n",
      "neutral     0.269     0.100     0.631     \n",
      "\n",
      "data/twitter-test3.txt (BERT+Linear): 0.662\n",
      "            positive  negative  neutral\n",
      "positive    0.744     0.034     0.222     \n",
      "negative    0.105     0.629     0.266     \n",
      "neutral     0.257     0.112     0.632     \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the bert model on the test set 1 This cell takes a long time to run on cpu\n",
    "\n",
    "to_test = testsets\n",
    "eval = True\n",
    "\n",
    "if (eval):\n",
    "\n",
    "    for testset in to_test:\n",
    "        bert_classifier.to(DEVICE)\n",
    "        bert_classifier.eval()\n",
    "        test_tweets = processed_tweets[testset]\n",
    "        test_labels = tweetlabels[testset]\n",
    "        test_ids = tweetids[testset]\n",
    "\n",
    "        X_test = bert_tokenizer(test_tweets, padding=True, truncation=True, max_length=40)\n",
    "        Y_test = torch.tensor([LABELS_IDS[label] for label in test_labels]).long()                                                                              \n",
    "\n",
    "        ids_test = torch.tensor(X_test['input_ids']).to(DEVICE)\n",
    "        masks_test = torch.tensor(X_test['attention_mask']).to(DEVICE)\n",
    "\n",
    "\n",
    "        with (torch.no_grad()):\n",
    "            test_pred = bert_classifier.to(DEVICE)(ids_test,masks_test)\n",
    "\n",
    "            test_pred_labels = test_pred.argmax(dim=-1)\n",
    "\n",
    "            id_preds = {id:LABELS[pred.item()] for id,pred in zip(test_ids,test_pred_labels)}\n",
    "        \n",
    "\n",
    "        evaluate(id_preds,join(data_dir,testset),'BERT+Linear')\n",
    "\n",
    "        confusion(id_preds,join(data_dir,testset),'BERT+Linear')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "57ca5a7ab4d2f818d9f010b877a57cd84b255ed81f71bf652dbe1fa9a8913657"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
